{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2 #to install it -> !pip install opencv-python\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function slipts the dataset into train and test folders:\n",
    "def prepare_data(filepath, src,dest):\n",
    "    classes_images = defaultdict(list)\n",
    "    with open(filepath, 'r') as txt:\n",
    "        paths = [read.strip() for read in txt.readlines()]\n",
    "        for p in paths:\n",
    "            food = p.split('/')\n",
    "            classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "    for food in classes_images.keys():\n",
    "        print(\"\\nCopying images into \",food)\n",
    "        if not os.path.exists(os.path.join(dest,food)):\n",
    "            os.makedirs(os.path.join(dest,food))\n",
    "        for i in classes_images[food]:\n",
    "            copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "    print(\"Copying Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train data...\n",
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  tiramisu\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n",
    "print(\"Creating train data...\")\n",
    "prepare_data('../food-images/image_desert/train.txt', '../food-images/image_desert', '../food-images/image_desert/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data...\n",
      "\n",
      "Copying images into  apple_pie\n",
      "\n",
      "Copying images into  cannoli\n",
      "\n",
      "Copying images into  carrot_cake\n",
      "\n",
      "Copying images into  cheesecake\n",
      "\n",
      "Copying images into  chocolate_cake\n",
      "\n",
      "Copying images into  chocolate_mousse\n",
      "\n",
      "Copying images into  ice_cream\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  strawberry_shortcake\n",
      "\n",
      "Copying images into  tiramisu\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\n",
    "print(\"Creating test data...\")\n",
    "prepare_data('../food-images/image_desert/test.txt', '../food-images/image_desert', '../food-images/image_desert/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in train folder\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'find: ‘train’: No such file or directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a11cfa5092b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of samples in train folder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"find train -type d -or -type f -printf '.' | wc -c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_total_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_total_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'find: ‘train’: No such file or directory'"
     ]
    }
   ],
   "source": [
    "# Check how many files are in the train folder\n",
    "print(\"Total number of samples in train folder\")\n",
    "train_total_samples = !find train -type d -or -type f -printf '.' | wc -c\n",
    "train_total_samples = int(train_total_samples[0])\n",
    "train_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in test folder\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'find: ‘test’: No such file or directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89399d9ab54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of samples in test folder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"find test -type d -or -type f -printf '.' | wc -c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_total_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_total_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'find: ‘test’: No such file or directory'"
     ]
    }
   ],
   "source": [
    "# Check how many files are in the test folder\n",
    "print(\"Total number of samples in test folder\")\n",
    "test_total_samples = !find test -type d -or -type f -printf '.' | wc -c\n",
    "test_total_samples = int(test_total_samples[0])\n",
    "test_total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know where the last dimension goes:\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know how many folders of images do we have, this will be our number of clases:\n",
    "a = !ls food-101/food-101/images\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLATES - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37500 images belonging to 50 classes.\n",
      "Found 12500 images belonging to 50 classes.\n",
      "WARNING:tensorflow:From <ipython-input-11-f20a93edb920>:66: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2343 steps, validate for 781 steps\n",
      "Epoch 1/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 4.2140 - accuracy: 0.0792\n",
      "Epoch 00001: val_loss improved from inf to 3.69098, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1986s 848ms/step - loss: 4.2139 - accuracy: 0.0792 - val_loss: 3.6910 - val_accuracy: 0.2214\n",
      "Epoch 2/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 3.4397 - accuracy: 0.2526\n",
      "Epoch 00002: val_loss improved from 3.69098 to 2.82870, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1973s 842ms/step - loss: 3.4397 - accuracy: 0.2526 - val_loss: 2.8287 - val_accuracy: 0.3850\n",
      "Epoch 3/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.8953 - accuracy: 0.3635\n",
      "Epoch 00003: val_loss improved from 2.82870 to 2.36894, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1889s 806ms/step - loss: 2.8952 - accuracy: 0.3635 - val_loss: 2.3689 - val_accuracy: 0.4855\n",
      "Epoch 4/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.5709 - accuracy: 0.4318\n",
      "Epoch 00004: val_loss improved from 2.36894 to 2.17608, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1882s 803ms/step - loss: 2.5710 - accuracy: 0.4318 - val_loss: 2.1761 - val_accuracy: 0.5216\n",
      "Epoch 5/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.3434 - accuracy: 0.4832\n",
      "Epoch 00005: val_loss improved from 2.17608 to 2.03175, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1833s 782ms/step - loss: 2.3432 - accuracy: 0.4833 - val_loss: 2.0318 - val_accuracy: 0.5530\n",
      "Epoch 6/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.1575 - accuracy: 0.5264\n",
      "Epoch 00006: val_loss improved from 2.03175 to 1.90973, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1845s 788ms/step - loss: 2.1575 - accuracy: 0.5264 - val_loss: 1.9097 - val_accuracy: 0.5871\n",
      "Epoch 7/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.9955 - accuracy: 0.5619\n",
      "Epoch 00007: val_loss improved from 1.90973 to 1.84084, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1819s 776ms/step - loss: 1.9954 - accuracy: 0.5619 - val_loss: 1.8408 - val_accuracy: 0.5983\n",
      "Epoch 8/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.8648 - accuracy: 0.5917\n",
      "Epoch 00008: val_loss improved from 1.84084 to 1.78043, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1766s 754ms/step - loss: 1.8646 - accuracy: 0.5917 - val_loss: 1.7804 - val_accuracy: 0.6131\n",
      "Epoch 9/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.7452 - accuracy: 0.6183\n",
      "Epoch 00009: val_loss improved from 1.78043 to 1.75823, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1775s 758ms/step - loss: 1.7453 - accuracy: 0.6183 - val_loss: 1.7582 - val_accuracy: 0.6185\n",
      "Epoch 10/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.6351 - accuracy: 0.6441\n",
      "Epoch 00010: val_loss improved from 1.75823 to 1.70435, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1785s 762ms/step - loss: 1.6350 - accuracy: 0.6441 - val_loss: 1.7044 - val_accuracy: 0.6308\n",
      "Epoch 11/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.5324 - accuracy: 0.6695\n",
      "Epoch 00011: val_loss improved from 1.70435 to 1.67003, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1866s 796ms/step - loss: 1.5323 - accuracy: 0.6694 - val_loss: 1.6700 - val_accuracy: 0.6366\n",
      "Epoch 12/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.4340 - accuracy: 0.6906\n",
      "Epoch 00012: val_loss improved from 1.67003 to 1.66209, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1771s 756ms/step - loss: 1.4342 - accuracy: 0.6905 - val_loss: 1.6621 - val_accuracy: 0.6399\n",
      "Epoch 13/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.3479 - accuracy: 0.7115\n",
      "Epoch 00013: val_loss improved from 1.66209 to 1.61894, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1933s 825ms/step - loss: 1.3477 - accuracy: 0.7115 - val_loss: 1.6189 - val_accuracy: 0.6467\n",
      "Epoch 14/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.2652 - accuracy: 0.7311\n",
      "Epoch 00014: val_loss improved from 1.61894 to 1.59999, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1768s 754ms/step - loss: 1.2653 - accuracy: 0.7310 - val_loss: 1.6000 - val_accuracy: 0.6520\n",
      "Epoch 15/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.1848 - accuracy: 0.7550\n",
      "Epoch 00015: val_loss did not improve from 1.59999\n",
      "2343/2343 [==============================] - 1863s 795ms/step - loss: 1.1848 - accuracy: 0.7550 - val_loss: 1.6119 - val_accuracy: 0.6538\n",
      "Epoch 16/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.1096 - accuracy: 0.7688\n",
      "Epoch 00016: val_loss improved from 1.59999 to 1.58513, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1897s 810ms/step - loss: 1.1096 - accuracy: 0.7688 - val_loss: 1.5851 - val_accuracy: 0.6560\n",
      "Epoch 17/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.7885\n",
      "Epoch 00017: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1859s 793ms/step - loss: 1.0361 - accuracy: 0.7885 - val_loss: 1.6040 - val_accuracy: 0.6542\n",
      "Epoch 18/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.9664 - accuracy: 0.8091\n",
      "Epoch 00018: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1799s 768ms/step - loss: 0.9665 - accuracy: 0.8090 - val_loss: 1.5860 - val_accuracy: 0.6618\n",
      "Epoch 19/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.9017 - accuracy: 0.8236\n",
      "Epoch 00019: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1819s 776ms/step - loss: 0.9016 - accuracy: 0.8236 - val_loss: 1.6177 - val_accuracy: 0.6511\n",
      "Epoch 20/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.8421 - accuracy: 0.8417\n",
      "Epoch 00020: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 2012s 859ms/step - loss: 0.8421 - accuracy: 0.8417 - val_loss: 1.6078 - val_accuracy: 0.6636\n"
     ]
    }
   ],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 50\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS =20\n",
    "\n",
    "\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = '../food-images/image_plates/train'\n",
    "validation_data_folder = '../food-images/image_plates/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 37500\n",
    "num_validation_samples = 12500\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_45class_salads.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('traning_total45_salads.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_45class_salads.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SALADS - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 5\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS =20\n",
    "\n",
    "labels = ['beet_salad', 'caesar_salad', 'caprese_salad', 'greek_salad', 'seaweed_salad']\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = 'food-images/image_salads/train'\n",
    "validation_data_folder = 'food-images/image_salads/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 3750\n",
    "num_validation_samples = 1250\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_salads.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('traning_salads.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_salads.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESERTS - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7500 images belonging to 10 classes.\n",
      "Found 2500 images belonging to 10 classes.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 468 steps, validate for 156 steps\n",
      "Epoch 1/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.4148 - accuracy: 0.1450\n",
      "Epoch 00001: val_loss improved from inf to 2.36626, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 425s 909ms/step - loss: 2.4141 - accuracy: 0.1455 - val_loss: 2.3663 - val_accuracy: 0.2356\n",
      "Epoch 2/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 2.1809 - accuracy: 0.2658\n",
      "Epoch 00002: val_loss improved from 2.36626 to 2.09188, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 401s 857ms/step - loss: 2.1806 - accuracy: 0.2656 - val_loss: 2.0919 - val_accuracy: 0.3482\n",
      "Epoch 3/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.9616 - accuracy: 0.3630\n",
      "Epoch 00003: val_loss improved from 2.09188 to 1.86022, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 404s 863ms/step - loss: 1.9614 - accuracy: 0.3630 - val_loss: 1.8602 - val_accuracy: 0.4191\n",
      "Epoch 4/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.7656 - accuracy: 0.4340\n",
      "Epoch 00004: val_loss improved from 1.86022 to 1.69257, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 404s 864ms/step - loss: 1.7657 - accuracy: 0.4341 - val_loss: 1.6926 - val_accuracy: 0.4700\n",
      "Epoch 5/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.6072 - accuracy: 0.4968\n",
      "Epoch 00005: val_loss improved from 1.69257 to 1.63334, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 403s 862ms/step - loss: 1.6079 - accuracy: 0.4964 - val_loss: 1.6333 - val_accuracy: 0.5028\n",
      "Epoch 6/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.4829 - accuracy: 0.5325\n",
      "Epoch 00006: val_loss improved from 1.63334 to 1.55889, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 403s 862ms/step - loss: 1.4823 - accuracy: 0.5329 - val_loss: 1.5589 - val_accuracy: 0.5329\n",
      "Epoch 7/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.3635 - accuracy: 0.5767\n",
      "Epoch 00007: val_loss improved from 1.55889 to 1.47721, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 402s 859ms/step - loss: 1.3629 - accuracy: 0.5767 - val_loss: 1.4772 - val_accuracy: 0.5505\n",
      "Epoch 8/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.2601 - accuracy: 0.6129\n",
      "Epoch 00008: val_loss improved from 1.47721 to 1.41113, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 401s 857ms/step - loss: 1.2605 - accuracy: 0.6125 - val_loss: 1.4111 - val_accuracy: 0.5817\n",
      "Epoch 9/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.1748 - accuracy: 0.6387\n",
      "Epoch 00009: val_loss did not improve from 1.41113\n",
      "468/468 [==============================] - 400s 854ms/step - loss: 1.1747 - accuracy: 0.6386 - val_loss: 1.4223 - val_accuracy: 0.5837\n",
      "Epoch 10/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 1.0815 - accuracy: 0.6691\n",
      "Epoch 00010: val_loss improved from 1.41113 to 1.40197, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 402s 859ms/step - loss: 1.0811 - accuracy: 0.6694 - val_loss: 1.4020 - val_accuracy: 0.5901\n",
      "Epoch 11/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.9974 - accuracy: 0.6970\n",
      "Epoch 00011: val_loss improved from 1.40197 to 1.36706, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 402s 860ms/step - loss: 0.9968 - accuracy: 0.6972 - val_loss: 1.3671 - val_accuracy: 0.6002\n",
      "Epoch 12/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.9154 - accuracy: 0.7299\n",
      "Epoch 00012: val_loss did not improve from 1.36706\n",
      "468/468 [==============================] - 400s 855ms/step - loss: 0.9166 - accuracy: 0.7296 - val_loss: 1.3740 - val_accuracy: 0.5938\n",
      "Epoch 13/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.8535 - accuracy: 0.7521\n",
      "Epoch 00013: val_loss improved from 1.36706 to 1.34030, saving model to best_model_9desert.hdf5\n",
      "468/468 [==============================] - 401s 856ms/step - loss: 0.8531 - accuracy: 0.7524 - val_loss: 1.3403 - val_accuracy: 0.5954\n",
      "Epoch 14/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.7704 - accuracy: 0.7816\n",
      "Epoch 00014: val_loss did not improve from 1.34030\n",
      "468/468 [==============================] - 399s 853ms/step - loss: 0.7701 - accuracy: 0.7818 - val_loss: 1.3608 - val_accuracy: 0.6030\n",
      "Epoch 15/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.7078 - accuracy: 0.8061\n",
      "Epoch 00015: val_loss did not improve from 1.34030\n",
      "468/468 [==============================] - 400s 854ms/step - loss: 0.7079 - accuracy: 0.8063 - val_loss: 1.3677 - val_accuracy: 0.6062\n",
      "Epoch 16/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.6514 - accuracy: 0.8291\n",
      "Epoch 00016: val_loss did not improve from 1.34030\n",
      "468/468 [==============================] - 401s 856ms/step - loss: 0.6516 - accuracy: 0.8292 - val_loss: 1.3976 - val_accuracy: 0.6090\n",
      "Epoch 17/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.5962 - accuracy: 0.8447\n",
      "Epoch 00017: val_loss did not improve from 1.34030\n",
      "468/468 [==============================] - 401s 856ms/step - loss: 0.5957 - accuracy: 0.8449 - val_loss: 1.3814 - val_accuracy: 0.6246\n",
      "Epoch 18/18\n",
      "467/468 [============================>.] - ETA: 0s - loss: 0.5335 - accuracy: 0.8719\n",
      "Epoch 00018: val_loss did not improve from 1.34030\n",
      "468/468 [==============================] - 400s 855ms/step - loss: 0.5336 - accuracy: 0.8719 - val_loss: 1.4247 - val_accuracy: 0.6138\n"
     ]
    }
   ],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 10\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 18\n",
    "\n",
    "\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = '../food-images/image_desert/train'\n",
    "validation_data_folder = '../food-images/image_desert/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 7500\n",
    "num_validation_samples = 2500\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_9desert.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('model9desert.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_9desert.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project_gpu]",
   "language": "python",
   "name": "conda-env-project_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
