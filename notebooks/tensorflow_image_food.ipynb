{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import cv2 #to install it -> !pip install opencv-python\n",
    "from tensorflow import keras\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "from shutil import copy\n",
    "from shutil import copytree, rmtree\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing import image\n",
    "import os\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras import models\n",
    "from pathlib import Path\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function slipts the dataset into train and test folders:\n",
    "def prepare_data(filepath, src,dest):\n",
    "    classes_images = defaultdict(list)\n",
    "    with open(filepath, 'r') as txt:\n",
    "        paths = [read.strip() for read in txt.readlines()]\n",
    "        for p in paths:\n",
    "            food = p.split('/')\n",
    "            classes_images[food[0]].append(food[1] + '.jpg')\n",
    "\n",
    "    for food in classes_images.keys():\n",
    "        print(\"\\nCopying images into \",food)\n",
    "        if not os.path.exists(os.path.join(dest,food)):\n",
    "            os.makedirs(os.path.join(dest,food))\n",
    "        for i in classes_images[food]:\n",
    "            copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
    "    print(\"Copying Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train data...\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "# Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n",
    "print(\"Creating train data...\")\n",
    "prepare_data('../food-images/image_plates/train.txt', '../food-images/image_plates', '../food-images/image_plates/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating test data...\n",
      "\n",
      "Copying images into  beef_tartare\n",
      "\n",
      "Copying images into  beet_salad\n",
      "\n",
      "Copying images into  breakfast_burrito\n",
      "\n",
      "Copying images into  caesar_salad\n",
      "\n",
      "Copying images into  caprese_salad\n",
      "\n",
      "Copying images into  ceviche\n",
      "\n",
      "Copying images into  chicken_curry\n",
      "\n",
      "Copying images into  chicken_quesadilla\n",
      "\n",
      "Copying images into  chicken_wings\n",
      "\n",
      "Copying images into  club_sandwich\n",
      "\n",
      "Copying images into  dumplings\n",
      "\n",
      "Copying images into  edamame\n",
      "\n",
      "Copying images into  falafel\n",
      "\n",
      "Copying images into  french_fries\n",
      "\n",
      "Copying images into  fried_calamari\n",
      "\n",
      "Copying images into  fried_rice\n",
      "\n",
      "Copying images into  greek_salad\n",
      "\n",
      "Copying images into  grilled_cheese_sandwich\n",
      "\n",
      "Copying images into  guacamole\n",
      "\n",
      "Copying images into  gyoza\n",
      "\n",
      "Copying images into  hamburger\n",
      "\n",
      "Copying images into  hot_dog\n",
      "\n",
      "Copying images into  hummus\n",
      "\n",
      "Copying images into  lasagna\n",
      "\n",
      "Copying images into  miso_soup\n",
      "\n",
      "Copying images into  mussels\n",
      "\n",
      "Copying images into  nachos\n",
      "\n",
      "Copying images into  omelette\n",
      "\n",
      "Copying images into  onion_rings\n",
      "\n",
      "Copying images into  oysters\n",
      "\n",
      "Copying images into  pad_thai\n",
      "\n",
      "Copying images into  paella\n",
      "\n",
      "Copying images into  panna_cotta\n",
      "\n",
      "Copying images into  pizza\n",
      "\n",
      "Copying images into  pork_chop\n",
      "\n",
      "Copying images into  prime_rib\n",
      "\n",
      "Copying images into  ramen\n",
      "\n",
      "Copying images into  ravioli\n",
      "\n",
      "Copying images into  risotto\n",
      "\n",
      "Copying images into  samosa\n",
      "\n",
      "Copying images into  sashimi\n",
      "\n",
      "Copying images into  seaweed_salad\n",
      "\n",
      "Copying images into  scallops\n",
      "\n",
      "Copying images into  spaghetti_bolognese\n",
      "\n",
      "Copying images into  spaghetti_carbonara\n",
      "\n",
      "Copying images into  spring_rolls\n",
      "\n",
      "Copying images into  steak\n",
      "\n",
      "Copying images into  sushi\n",
      "\n",
      "Copying images into  tacos\n",
      "\n",
      "Copying images into  tuna_tartare\n",
      "Copying Done!\n"
     ]
    }
   ],
   "source": [
    "# Prepare test data by copying images from food-101/images to food-101/test using the file test.txt\n",
    "print(\"Creating test data...\")\n",
    "prepare_data('../food-images/image_plates/test.txt', '../food-images/image_plates', '../food-images/image_plates/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in train folder\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'find: ‘train’: No such file or directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-a11cfa5092b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of samples in train folder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtrain_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"find train -type d -or -type f -printf '.' | wc -c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtrain_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_total_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtrain_total_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'find: ‘train’: No such file or directory'"
     ]
    }
   ],
   "source": [
    "# Check how many files are in the train folder\n",
    "print(\"Total number of samples in train folder\")\n",
    "train_total_samples = !find train -type d -or -type f -printf '.' | wc -c\n",
    "train_total_samples = int(train_total_samples[0])\n",
    "train_total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of samples in test folder\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'find: ‘test’: No such file or directory'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-89399d9ab54f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Total number of samples in test folder\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetoutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"find test -type d -or -type f -printf '.' | wc -c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_total_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_total_samples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mtest_total_samples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'find: ‘test’: No such file or directory'"
     ]
    }
   ],
   "source": [
    "# Check how many files are in the test folder\n",
    "print(\"Total number of samples in test folder\")\n",
    "test_total_samples = !find test -type d -or -type f -printf '.' | wc -c\n",
    "test_total_samples = int(test_total_samples[0])\n",
    "test_total_samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know where the last dimension goes:\n",
    "K.image_data_format()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To know how many folders of images do we have, this will be our number of clases:\n",
    "a = !ls food-101/food-101/images\n",
    "len(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PLATES - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 37500 images belonging to 50 classes.\n",
      "Found 12500 images belonging to 50 classes.\n",
      "WARNING:tensorflow:From <ipython-input-11-f20a93edb920>:66: Model.fit_generator (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use Model.fit, which supports generators.\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "WARNING:tensorflow:sample_weight modes were coerced from\n",
      "  ...\n",
      "    to  \n",
      "  ['...']\n",
      "Train for 2343 steps, validate for 781 steps\n",
      "Epoch 1/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 4.2140 - accuracy: 0.0792\n",
      "Epoch 00001: val_loss improved from inf to 3.69098, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1986s 848ms/step - loss: 4.2139 - accuracy: 0.0792 - val_loss: 3.6910 - val_accuracy: 0.2214\n",
      "Epoch 2/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 3.4397 - accuracy: 0.2526\n",
      "Epoch 00002: val_loss improved from 3.69098 to 2.82870, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1973s 842ms/step - loss: 3.4397 - accuracy: 0.2526 - val_loss: 2.8287 - val_accuracy: 0.3850\n",
      "Epoch 3/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.8953 - accuracy: 0.3635\n",
      "Epoch 00003: val_loss improved from 2.82870 to 2.36894, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1889s 806ms/step - loss: 2.8952 - accuracy: 0.3635 - val_loss: 2.3689 - val_accuracy: 0.4855\n",
      "Epoch 4/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.5709 - accuracy: 0.4318\n",
      "Epoch 00004: val_loss improved from 2.36894 to 2.17608, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1882s 803ms/step - loss: 2.5710 - accuracy: 0.4318 - val_loss: 2.1761 - val_accuracy: 0.5216\n",
      "Epoch 5/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.3434 - accuracy: 0.4832\n",
      "Epoch 00005: val_loss improved from 2.17608 to 2.03175, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1833s 782ms/step - loss: 2.3432 - accuracy: 0.4833 - val_loss: 2.0318 - val_accuracy: 0.5530\n",
      "Epoch 6/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 2.1575 - accuracy: 0.5264\n",
      "Epoch 00006: val_loss improved from 2.03175 to 1.90973, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1845s 788ms/step - loss: 2.1575 - accuracy: 0.5264 - val_loss: 1.9097 - val_accuracy: 0.5871\n",
      "Epoch 7/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.9955 - accuracy: 0.5619\n",
      "Epoch 00007: val_loss improved from 1.90973 to 1.84084, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1819s 776ms/step - loss: 1.9954 - accuracy: 0.5619 - val_loss: 1.8408 - val_accuracy: 0.5983\n",
      "Epoch 8/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.8648 - accuracy: 0.5917\n",
      "Epoch 00008: val_loss improved from 1.84084 to 1.78043, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1766s 754ms/step - loss: 1.8646 - accuracy: 0.5917 - val_loss: 1.7804 - val_accuracy: 0.6131\n",
      "Epoch 9/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.7452 - accuracy: 0.6183\n",
      "Epoch 00009: val_loss improved from 1.78043 to 1.75823, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1775s 758ms/step - loss: 1.7453 - accuracy: 0.6183 - val_loss: 1.7582 - val_accuracy: 0.6185\n",
      "Epoch 10/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.6351 - accuracy: 0.6441\n",
      "Epoch 00010: val_loss improved from 1.75823 to 1.70435, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1785s 762ms/step - loss: 1.6350 - accuracy: 0.6441 - val_loss: 1.7044 - val_accuracy: 0.6308\n",
      "Epoch 11/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.5324 - accuracy: 0.6695\n",
      "Epoch 00011: val_loss improved from 1.70435 to 1.67003, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1866s 796ms/step - loss: 1.5323 - accuracy: 0.6694 - val_loss: 1.6700 - val_accuracy: 0.6366\n",
      "Epoch 12/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.4340 - accuracy: 0.6906\n",
      "Epoch 00012: val_loss improved from 1.67003 to 1.66209, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1771s 756ms/step - loss: 1.4342 - accuracy: 0.6905 - val_loss: 1.6621 - val_accuracy: 0.6399\n",
      "Epoch 13/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.3479 - accuracy: 0.7115\n",
      "Epoch 00013: val_loss improved from 1.66209 to 1.61894, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1933s 825ms/step - loss: 1.3477 - accuracy: 0.7115 - val_loss: 1.6189 - val_accuracy: 0.6467\n",
      "Epoch 14/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.2652 - accuracy: 0.7311\n",
      "Epoch 00014: val_loss improved from 1.61894 to 1.59999, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1768s 754ms/step - loss: 1.2653 - accuracy: 0.7310 - val_loss: 1.6000 - val_accuracy: 0.6520\n",
      "Epoch 15/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.1848 - accuracy: 0.7550\n",
      "Epoch 00015: val_loss did not improve from 1.59999\n",
      "2343/2343 [==============================] - 1863s 795ms/step - loss: 1.1848 - accuracy: 0.7550 - val_loss: 1.6119 - val_accuracy: 0.6538\n",
      "Epoch 16/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.1096 - accuracy: 0.7688\n",
      "Epoch 00016: val_loss improved from 1.59999 to 1.58513, saving model to best_model_45class_salads.hdf5\n",
      "2343/2343 [==============================] - 1897s 810ms/step - loss: 1.1096 - accuracy: 0.7688 - val_loss: 1.5851 - val_accuracy: 0.6560\n",
      "Epoch 17/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 1.0359 - accuracy: 0.7885\n",
      "Epoch 00017: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1859s 793ms/step - loss: 1.0361 - accuracy: 0.7885 - val_loss: 1.6040 - val_accuracy: 0.6542\n",
      "Epoch 18/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.9664 - accuracy: 0.8091\n",
      "Epoch 00018: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1799s 768ms/step - loss: 0.9665 - accuracy: 0.8090 - val_loss: 1.5860 - val_accuracy: 0.6618\n",
      "Epoch 19/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.9017 - accuracy: 0.8236\n",
      "Epoch 00019: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 1819s 776ms/step - loss: 0.9016 - accuracy: 0.8236 - val_loss: 1.6177 - val_accuracy: 0.6511\n",
      "Epoch 20/20\n",
      "2342/2343 [============================>.] - ETA: 0s - loss: 0.8421 - accuracy: 0.8417\n",
      "Epoch 00020: val_loss did not improve from 1.58513\n",
      "2343/2343 [==============================] - 2012s 859ms/step - loss: 0.8421 - accuracy: 0.8417 - val_loss: 1.6078 - val_accuracy: 0.6636\n"
     ]
    }
   ],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 50\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS =20\n",
    "\n",
    "\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = '../food-images/image_plates/train'\n",
    "validation_data_folder = '../food-images/image_plates/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 37500\n",
    "num_validation_samples = 12500\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_45class_salads.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('traning_total45_salads.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_45class_salads.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SALADS - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 5\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS =20\n",
    "\n",
    "labels = ['beet_salad', 'caesar_salad', 'caprese_salad', 'greek_salad', 'seaweed_salad']\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = 'food-images/image_salads/train'\n",
    "validation_data_folder = 'food-images/image_salads/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 3750\n",
    "num_validation_samples = 1250\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_salads.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('traning_salads.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_salads.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DESERTS - Model classification training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Clearing the session removes all the nodes left over from previous models, freeing memory and preventing slowdown.\n",
    "K.clear_session()\n",
    "\n",
    "\n",
    "#Defining number of clases, num of pixels, batch_size-> dataset divided in blocks to train\n",
    "#Epochs->num of times the model runs the same dataset\n",
    "NUM_CLASSES = 9\n",
    "IMG_ROWS, IMG_COLS = 128, 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 18\n",
    "\n",
    "\n",
    "\n",
    "#Folder to take train and test data into:\n",
    "train_data_folder = 'food-images/image_desert/train'\n",
    "validation_data_folder = 'food-images/image_desert/test'\n",
    "\n",
    "\n",
    "#defining the total number of train and validation images:\n",
    "num_train_samples = 6750\n",
    "num_validation_samples = 2250\n",
    "\n",
    "train_datagenerator = ImageDataGenerator(rescale=1. / 255,\n",
    "                                         shear_range=0.2,\n",
    "                                         zoom_range=0.2,\n",
    "                                         horizontal_flip=True)\n",
    "\n",
    "\n",
    "test_datagenerator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "\n",
    "train_generator = train_datagenerator.flow_from_directory(train_data_folder,\n",
    "                                                         target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                         batch_size=BATCH_SIZE,\n",
    "                                                         class_mode='categorical')\n",
    "\n",
    "\n",
    "test_generator = test_datagenerator.flow_from_directory(validation_data_folder,\n",
    "                                                       target_size=(IMG_ROWS, IMG_COLS),\n",
    "                                                       batch_size=BATCH_SIZE,\n",
    "                                                       class_mode='categorical')\n",
    "\n",
    "\n",
    "# create the base pre-trained model:\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=(IMG_ROWS, IMG_COLS, 3))\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = Dropout(0.2)(x) #use to reduce overfiting\n",
    "\n",
    "\n",
    "predictions = Dense(NUM_CLASSES, kernel_regularizer=regularizers.l2(0.005), activation='softmax')(x)\n",
    "\n",
    "\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "checkpointer = ModelCheckpoint(filepath='best_model_9desert.hdf5', verbose=1, save_best_only=True)\n",
    "csv_logger = CSVLogger('model9desert.log')\n",
    "\n",
    "history = model.fit_generator(train_generator,\n",
    "                    steps_per_epoch = num_train_samples // BATCH_SIZE,\n",
    "                    validation_data=test_generator,\n",
    "                    validation_steps=num_validation_samples // BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    verbose=1,\n",
    "                    callbacks=[csv_logger, checkpointer])\n",
    "\n",
    "model.save('model_trained_9desert.hdf5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:project_gpu]",
   "language": "python",
   "name": "conda-env-project_gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
